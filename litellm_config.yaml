# LiteLLM Proxy Configuration
# Proxies all Modal vLLM GPU server endpoints
#
# Architecture: GPU-based multi-model servers
#   - Each GPU endpoint serves ALL models (gemma-3-12b, gemma-3-27b, qwen3-vl-30b)
#   - 5 endpoints total: vllm-l40s, vllm-a100, vllm-h100, vllm-h200, vllm-b200
#
# Environment variables required:
#   MODAL_WORKSPACE - Your Modal workspace name
#   VLLM_API_KEY - API key for vLLM endpoints (optional)
#
# Usage:
#   docker compose up -d
#   curl http://localhost:4000/v1/models
#   curl http://localhost:4000/v1/chat/completions -d '{"model": "h100/gemma-3-12b", ...}'

model_list:
  # ============================================
  # Gemma 3 12B on different GPUs
  # ============================================
  - model_name: l40s/gemma-3-12b
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: os.environ/VLLM_L40S_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: a100/gemma-3-12b
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: os.environ/VLLM_A100_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: h100/gemma-3-12b
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: os.environ/VLLM_H100_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: h200/gemma-3-12b
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: os.environ/VLLM_H200_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: b200/gemma-3-12b
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: os.environ/VLLM_B200_API_BASE
      api_key: os.environ/VLLM_API_KEY

  # ============================================
  # Gemma 3 27B on different GPUs
  # ============================================
  - model_name: l40s/gemma-3-27b
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: os.environ/VLLM_L40S_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: a100/gemma-3-27b
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: os.environ/VLLM_A100_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: h100/gemma-3-27b
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: os.environ/VLLM_H100_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: h200/gemma-3-27b
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: os.environ/VLLM_H200_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: b200/gemma-3-27b
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: os.environ/VLLM_B200_API_BASE
      api_key: os.environ/VLLM_API_KEY

  # ============================================
  # Qwen3 VL 30B on different GPUs
  # ============================================
  - model_name: l40s/qwen3-vl-30b
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: os.environ/VLLM_L40S_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: a100/qwen3-vl-30b
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: os.environ/VLLM_A100_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: h100/qwen3-vl-30b
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: os.environ/VLLM_H100_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: h200/qwen3-vl-30b
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: os.environ/VLLM_H200_API_BASE
      api_key: os.environ/VLLM_API_KEY

  - model_name: b200/qwen3-vl-30b
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: os.environ/VLLM_B200_API_BASE
      api_key: os.environ/VLLM_API_KEY

# General settings
general_settings:
  master_key: os.environ/LITELLM_API_KEY
  telemetry: false
  disable_spend_logs: true
  disable_spend_updates: true
  
litellm_settings:
  drop_params: true  # Drop unsupported params instead of erroring
  request_timeout: 600  # 10 minute timeout for cold starts
  set_verbose: false
  turn_off_message_logging: true
  disable_end_user_cost_tracking: true
  disable_spend_logs: true
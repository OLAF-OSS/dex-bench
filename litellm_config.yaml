# LiteLLM Proxy Configuration
# Proxies all Modal vLLM server endpoints
#
# Environment variables required:
#   MODAL_WORKSPACE - Your Modal workspace name
#
# Usage:
#   docker compose up -d
#   curl http://localhost:4000/v1/models

model_list:
  # ============================================
  # Gemma 3 12B (google/gemma-3-12b-it)
  # ============================================
  - model_name: gemma-3-12b-l40s
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-12b-l40s-serve.modal.run/v1

  - model_name: gemma-3-12b-a100
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-12b-a100-serve.modal.run/v1

  - model_name: gemma-3-12b-h100
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-12b-h100-serve.modal.run/v1

  - model_name: gemma-3-12b-h200
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-12b-h200-serve.modal.run/v1

  - model_name: gemma-3-12b-b200
    litellm_params:
      model: openai/google/gemma-3-12b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-12b-b200-serve.modal.run/v1

  # ============================================
  # Gemma 3 27B (google/gemma-3-27b-it)
  # ============================================
  - model_name: gemma-3-27b-l40s
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-27b-l40s-serve.modal.run/v1

  - model_name: gemma-3-27b-a100
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-27b-a100-serve.modal.run/v1

  - model_name: gemma-3-27b-h100
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-27b-h100-serve.modal.run/v1

  - model_name: gemma-3-27b-h200
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-27b-h200-serve.modal.run/v1

  - model_name: gemma-3-27b-b200
    litellm_params:
      model: openai/google/gemma-3-27b-it
      api_base: https://${MODAL_WORKSPACE}--vllm-gemma-3-27b-b200-serve.modal.run/v1

  # ============================================
  # Qwen3 VL 30B (qwen/qwen3-vl-30b-a3b-instruct)
  # ============================================
  - model_name: qwen3-vl-30b-l40s
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: https://${MODAL_WORKSPACE}--vllm-qwen3-vl-30b-l40s-serve.modal.run/v1

  - model_name: qwen3-vl-30b-a100
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: https://${MODAL_WORKSPACE}--vllm-qwen3-vl-30b-a100-serve.modal.run/v1

  - model_name: qwen3-vl-30b-h100
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: https://${MODAL_WORKSPACE}--vllm-qwen3-vl-30b-h100-serve.modal.run/v1

  - model_name: qwen3-vl-30b-h200
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: https://${MODAL_WORKSPACE}--vllm-qwen3-vl-30b-h200-serve.modal.run/v1

  - model_name: qwen3-vl-30b-b200
    litellm_params:
      model: openai/qwen/qwen3-vl-30b-a3b-instruct
      api_base: https://${MODAL_WORKSPACE}--vllm-qwen3-vl-30b-b200-serve.modal.run/v1

# General settings
general_settings:
  master_key: null  # No authentication required by default
  
litellm_settings:
  drop_params: true  # Drop unsupported params instead of erroring
  request_timeout: 600  # 10 minute timeout for cold starts


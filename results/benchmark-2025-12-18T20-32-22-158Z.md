# Benchmark Results

**Run ID:** `benchmark-2025-12-18T20-32-22-158Z`
**Timestamp:** 12/18/2025, 8:32:22 PM
**Models:** 2
**Documents:** 3
**Categories:** summarization

## Table of Contents

- [Summarization Benchmark](#summarization-benchmark)

## Summarization Benchmark

Tests LLM summarization capabilities with document analysis.

### Results

| Model | Document | Doc Tokens | Time | Input Tokens | Output Tokens | Total Tokens | Tok/s | Status |
|-------|----------|------------|------|--------------|---------------|--------------|-------|--------|
| gemma-3-27b/b200 | transformers-paper | 10,200 | 33.4s | 10,297 | 283 | 10,580 | 8.5 | ‚úÖ |
| gemma-3-27b/b200 | bitcoin-paper | 4,626 | 34.5s | 4,722 | 319 | 5,041 | 9.2 | ‚úÖ |
| gemma-3-27b/b200 | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 38.1s | 41,768 | 325 | 42,093 | 8.5 | ‚úÖ |
| qwen3-vl-30b/b200 | bitcoin-paper | 4,626 | 28.6s | 4,722 | 332 | 5,054 | 11.6 | ‚úÖ |
| qwen3-vl-30b/b200 | transformers-paper | 10,200 | 29.4s | 10,297 | 271 | 10,568 | 9.2 | ‚úÖ |
| qwen3-vl-30b/b200 | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 27.7s | 41,768 | 326 | 42,094 | 11.7 | ‚úÖ |

### Statistics

- **Total Duration:** 3m 12s
- **Average Duration:** 32s
- **Total Input Tokens:** 113,574
- **Total Output Tokens:** 1,856
- **Average Tokens/s:** 9.8
- **‚ö° Fastest:** qwen3-vl-30b/b200 on Directorate A ‚Äì Expenditure ‚Äì Operations.md (27.7s)
- **üê¢ Slowest:** gemma-3-27b/b200 on Directorate A ‚Äì Expenditure ‚Äì Operations.md (38.1s)

### Model Averages

| Model | Average Time |
|-------|--------------|
| qwen3-vl-30b/b200 | 28.6s |
| gemma-3-27b/b200 | 35.4s |

### Summaries

<details>
<summary><strong>gemma-3-27b/b200</strong> ‚Üí transformers-paper</summary>

This paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation, based entirely on attention mechanisms. It achieves state-of-the-art results on English-to-German (BLEU score of 28.4) and English-to-French (BLEU score of 41.8) translation, surpassing previous models in quality and training efficiency. The Transformer's parallelizable nature reduces training time to 12 hours on eight P100 GPUs. Key innovations include scaled dot-product attention, multi-head attention, and positional encoding. The model also demonstrates generalizability to English constituency parsing and provides publicly available code with visualizations of attention mechanisms.

</details>

<details>
<summary><strong>gemma-3-27b/b200</strong> ‚Üí bitcoin-paper</summary>

This document, titled ‚ÄúBitcoin: A Peer-to-Peer Electronic Cash System‚Äù by Satoshi Nakamoto, proposes a solution to creating a peer-to-peer electronic cash system without trusted third parties, addressing the double-spending problem. It details a network that timestamps transactions using proof-of-work, forming a secure and unchangeable record maintained by a distributed network. The system uses digital signatures, distributed timestamp servers, and incentive mechanisms like coin distribution and transaction fees. It also discusses disk space management with Merkle Trees, simplified payment verification, privacy considerations, and mathematical analysis demonstrating the system's robustness against attacks.

</details>

<details>
<summary><strong>gemma-3-27b/b200</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This OLAF investigation report details findings regarding potential misconduct within the European Border and Coast Guard Agency (FRONTEX), specifically concerning allegations of involvement in, or cover-up of, illegal pushbacks of migrants. The investigation, initiated in late 2020, focused on the actions of numerous FRONTEX personnel and examined incidents in the Aegean Sea and elsewhere. The report reveals failures to follow procedures, a lack of loyalty to EU principles, and deficiencies in managerial responsibilities, hindering FRONTEX‚Äôs ability to uphold fundamental rights. Key findings include downplaying incident reports, avoiding witnessing illegal activities, and a lack of transparency with the Fundamental Rights Officer (FRO). While direct involvement in pushbacks was difficult to definitively prove, the report concludes that the actions compromised FRONTEX‚Äôs commitment to fundamental rights and its overall integrity, identifying a need for improved oversight and accountability.

</details>

<details>
<summary><strong>qwen3-vl-30b/b200</strong> ‚Üí bitcoin-paper</summary>

The document "Bitcoin: A Peer-to-Peer Electronic Cash System" by Satoshi Nakamoto proposes a decentralized electronic cash system that eliminates the need for trusted third parties. It addresses the double-spending problem by using a peer-to-peer network to timestamp transactions through a chain of proof-of-work, creating a secure and immutable ledger. Transactions are verified using digital signatures, and the system relies on a distributed network of nodes to maintain the longest valid chain, provided honest nodes control the majority of computational power. The proof-of-work mechanism, inspired by Hashcash, ensures that altering the blockchain is computationally expensive. The system includes mechanisms for transaction broadcasting, block creation, and validation, along with incentives for nodes through newly created coins and transaction fees. It also incorporates Merkle Trees for efficient storage and simplified payment verification for lightweight users. A mathematical analysis demonstrates the system's security, showing that the likelihood of a successful attack decreases exponentially with each additional block.

</details>

<details>
<summary><strong>qwen3-vl-30b/b200</strong> ‚Üí transformers-paper</summary>

This paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks such as machine translation, which relies entirely on attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks, with BLEU scores of 28.4 and 41.8 respectively, outperforming previous models in both quality and training efficiency. The model's architecture consists of stacked self-attention and point-wise feed-forward layers in both the encoder and decoder, enabling significant parallelization and faster training. Key innovations include scaled dot-product attention and multi-head attention, which allow the model to capture dependencies between input and output sequences without the sequential constraints of recurrent networks or the distance limitations of convolutional networks. The paper details components such as positional encoding to account for sequence order and provides a comparative analysis of self-attention versus recurrent and convolutional layers, highlighting advantages in computational complexity, parallelization, and handling long-range dependencies. The Transformer also demonstrates strong generalization, achieving competitive results on English constituency parsing with limited training data. The authors have made their code publicly available to support further research.

</details>

<details>
<summary><strong>qwen3-vl-30b/b200</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This OLAF investigation report examines allegations of misconduct within the European Border and Coast Guard Agency (FRONTEX), focusing on potential involvement in or cover-up of illegal pushbacks of migrants. Initiated in late 2020, the investigation uncovered a pattern of procedural failures, lack of loyalty to EU principles, and managerial deficiencies, particularly in the Aegean Sea. Key findings include the deliberate downplaying of serious incident reports, decisions to avoid witnessing illegal activities, and a lack of transparency with the FRONTEX Fundamental Rights Officer. While several concerning incidents were confirmed, definitive proof of direct FRONTEX involvement was limited due to insufficient documentation and conflicting accounts. The report concludes that these actions compromised FRONTEX's commitment to fundamental rights and integrity, calling for improved oversight and accountability. The findings are based on interviews, document analysis, and digital forensics, providing a comprehensive account of the alleged misconduct and its implications.

</details>

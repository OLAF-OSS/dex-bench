# Benchmark Results

**Run ID:** `benchmark-2025-12-17T12-25-42-156Z`
**Timestamp:** 12/17/2025, 12:25:42 PM
**Models:** 4
**Documents:** 3
**Categories:** summarization

## Table of Contents

- [Summarization Benchmark](#summarization-benchmark)

## Summarization Benchmark

Tests LLM summarization capabilities with document analysis.

### Results

| Model | Document | Doc Tokens | Time | Input Tokens | Output Tokens | Total Tokens | Tok/s | Status |
|-------|----------|------------|------|--------------|---------------|--------------|-------|--------|
| gpt-oss-20b | bitcoin-paper | 4,626 | 5.9s | 4,722 | 550 | 5,272 | 93.1 | âœ… |
| gpt-oss-20b | Directorate A â€“ Expenditure â€“ Operations | 41,672 | 13.1s | 41,768 | 686 | 42,454 | 52.0 | âœ… |
| gemma-3-12b-it | transformers-paper | 10,200 | 21.5s | 10,297 | 828 | 11,125 | 38.4 | âœ… |
| gemma-3-27b-it | bitcoin-paper | 4,626 | 23.5s | 4,722 | 783 | 5,505 | 33.3 | âœ… |
| gemma-3-12b-it | bitcoin-paper | 4,626 | 1m 16.9s | 4,722 | 457 | 5,179 | 5.9 | âœ… |
| gemma-3-12b-it | Directorate A â€“ Expenditure â€“ Operations | 41,672 | 1m 20.2s | 41,768 | 1,514 | 43,282 | 18.9 | âœ… |
| gemma-3-27b-it | Directorate A â€“ Expenditure â€“ Operations | 41,672 | 21.6s | 41,768 | 1,343 | 43,111 | 62.0 | âœ… |
| qwen3-vl-30b-a3b-instruct | bitcoin-paper | 4,626 | 5.7s | 4,722 | 482 | 5,204 | 84.1 | âœ… |
| gemma-3-27b-it | transformers-paper | 10,200 | 49.3s | 10,297 | 444 | 10,741 | 9.0 | âœ… |
| gpt-oss-20b | transformers-paper | 10,200 | 1m 42.2s | 10,297 | 1,883 | 12,180 | 18.4 | âœ… |
| qwen3-vl-30b-a3b-instruct | transformers-paper | 10,200 | 6.6s | 10,297 | 464 | 10,761 | 69.7 | âœ… |
| qwen3-vl-30b-a3b-instruct | Directorate A â€“ Expenditure â€“ Operations | 41,672 | 13.6s | 41,768 | 705 | 42,473 | 51.6 | âœ… |

### Statistics

- **Total Duration:** 7m 0.8s
- **Average Duration:** 35s
- **Total Input Tokens:** 227,148
- **Total Output Tokens:** 10,139
- **Average Tokens/s:** 44.7
- **âš¡ Fastest:** qwen3-vl-30b-a3b-instruct on bitcoin-paper.md (5.7s)
- **ğŸ¢ Slowest:** gpt-oss-20b on transformers-paper.md (1m 42.2s)

### Model Averages

| Model | Average Time |
|-------|--------------|
| qwen3-vl-30b-a3b-instruct | 8.6s |
| gemma-3-27b-it | 31.5s |
| gpt-oss-20b | 40.4s |
| gemma-3-12b-it | 59.6s |

### Summaries

<details>
<summary><strong>gpt-oss-20b</strong> â†’ bitcoin-paper</summary>

The Bitcoin whitepaper presents a decentralized electronic cash system that eliminates the need for trusted third parties. It argues that while digital signatures can prove ownership, they cannot prevent doubleâ€‘spending without a central authority. To solve this, the authors propose a peerâ€‘toâ€‘peer network that uses proofâ€‘ofâ€‘work to create a tamperâ€‘resistant, publicly verifiable chain of transactions. Each block contains a set of transactions, a hash of the previous block, and a computational puzzle (SHAâ€‘256 hash with leading zero bits). The longest chain, which represents the greatest cumulative proofâ€‘ofâ€‘work, is accepted as the canonical history, ensuring that honest nodes with majority CPU power will outpace any attacker.

Key mechanisms include:  
- Timestamping and chaining: Transactions are broadcast, grouped into blocks, and each block is timestamped by hashing it into the chain, creating an immutable ledger.  
- Proofâ€‘ofâ€‘work: Nodes expend CPU cycles to find a nonce that satisfies the difficulty target, making it computationally impractical to alter past blocks.  
- Incentives: The first transaction in each block creates a new coin (mining reward), and transaction fees are added to the reward once the coin supply stabilizes, providing a selfâ€‘sustaining incentive for honest participation.  
- Merkle trees: Transactions within a block are hashed into a Merkle root, allowing efficient pruning of spent transactions and reducing storage requirements.  
- Simplified payment verification: Users can verify a transaction by downloading only block headers and a Merkle branch, reducing the need to run a full node while still relying on the longest chain for security.

The paper also discusses privacy (using new key pairs per transaction), transaction structure (multiple inputs/outputs for splitting and combining value), and the probability analysis of an attacker catching up with the honest chain. The analysis shows that the chance of a successful doubleâ€‘spend drops exponentially with the number of blocks added after the transaction, providing a quantifiable security margin.

In conclusion, Bitcoin offers a robust, trustless electronic cash system that relies on cryptographic proof and distributed consensus. Its design is intentionally simple and unstructured, enabling nodes to join or leave freely while maintaining a secure, immutable ledger through proofâ€‘ofâ€‘work and the longestâ€‘chain rule.

</details>

<details>
<summary><strong>gpt-oss-20b</strong> â†’ Directorate A â€“ Expenditure â€“ Operations</summary>

The OLAF final report (Caseâ€¯OC/2021/0451/A1) investigates alleged misconduct and irregularities within the European Border and Coast Guard Agency (FRONTEX), focusing on possible illegal pushbacks of migrants, internal mismanagement, and violations of fundamentalâ€‘rights obligations. The investigation was opened on 3â€¯Mayâ€¯2021 as a split of an earlier case (OC/2020/0866/A1) and aimed to assess the conduct of senior FRONTEX officials, the handling of serious incidents, and the agencyâ€™s compliance with its legal and ethical duties.

Key findings show that several FRONTEX managersâ€”most notably Klausâ€‘Michaelâ€¯Staude, Joachimâ€¯Ringâ€¯B.Eng., Basilioâ€¯Villenaâ€‘Carbajo, Reeceâ€¯Morgan, GÃ¼ntherâ€¯Heser, and othersâ€”failed to follow standard operating procedures for serious incident reporting, deliberately excluded the Fundamental Rights Officer (FRO) from investigations, and obstructed the reporting chain. Evidence also indicates intimidation and harassment of staff, micromanagement that stifled reporting lines, conflicts of interest in recruitment, and procurement irregularities. The report documents multiple instances where FRONTEX assets were allegedly involved in or covered up illegal pushbacks, particularly in the Aegean Sea, and where the agencyâ€™s response was delayed or inadequate.

The investigation concluded that the allegations are proven. The misconduct of the identified individuals hindered FRONTEXâ€™s ability to monitor and protect fundamental rights, undermining the agencyâ€™s mandate under Regulation (EU)â€¯2019/1896. OLAF found that serious incidents were misclassified (often as Categoryâ€¯2 instead of Categoryâ€¯4), that the FRO was excluded from critical information, and that followâ€‘up actions were insufficient or absent. The report also highlights the lack of timely communication with the FRO, the use of classification and publicâ€‘accessâ€‘toâ€‘documents justifications to withhold evidence, and the overall failure to uphold the agencyâ€™s legal and ethical responsibilities. The findings underscore a systemic failure within FRONTEX to respect fundamental rights and to conduct its operations transparently and lawfully.

</details>

<details>
<summary><strong>gemma-3-12b-it</strong> â†’ transformers-paper</summary>

The paper introduces the Transformer, a novel sequenceâ€‘toâ€‘sequence architecture that replaces recurrent and convolutional layers with a stack of multiâ€‘head selfâ€‘attention modules. The encoder and decoder each consist of six identical layers; each layer contains a multiâ€‘head selfâ€‘attention subâ€‘layer followed by a positionâ€‘wise feedâ€‘forward network, with residual connections and layer normalization. The model uses 512â€‘dimensional embeddings, 2048â€‘dimensional feedâ€‘forward hidden layers, and eight attention heads (each with 64â€‘dimensional keys, values, and queries). Positional information is injected via fixed sinusoidal encodings, allowing the network to learn relative positions without learned embeddings.

Training on the WMTâ€¯2014 Englishâ€‘toâ€‘German and Englishâ€‘toâ€‘French corpora, the Transformer achieves stateâ€‘ofâ€‘theâ€‘art BLEU scores while requiring far less compute. The base model (trained for 12â€¯h on 8â€¯P100 GPUs) attains 27.3â€¯BLEU (German) and 38.1â€¯BLEU (French), surpassing all previous singleâ€‘model results. The larger â€œbigâ€ model, trained for 3.5â€¯days, reaches 28.4â€¯BLEU on German and 41.8â€¯BLEU on French, outperforming ensembles and reducing training FLOPs by an order of magnitude compared to prior work. Key training details include Adam optimization with a warmâ€‘up schedule (4000 steps), dropout (0.1 on the base, 0.3 on the big model), and label smoothing (Îµâ‚—â‚›â€¯=â€¯0.1).

The authors also demonstrate the Transformerâ€™s versatility by applying it to English constituency parsing. A 4â€‘layer Transformer (d_modelâ€¯=â€¯1024) trained on the Penn Treebank achieves 91.3â€¯F1 on the WSJâ€¯23 test set, and 92.7â€¯F1 in a semiâ€‘supervised setting, matching or surpassing the best RNNâ€‘based parsers. The paper concludes that selfâ€‘attention yields highly parallelizable models with constantâ€‘length dependency paths, enabling efficient training and strong generalization across tasks, and outlines future directions such as local attention mechanisms and multimodal extensions.

</details>

<details>
<summary><strong>gemma-3-27b-it</strong> â†’ bitcoin-paper</summary>

Satoshi Nakamotoâ€™s whitepaper proposes a decentralized electronic cash system that eliminates the need for a trusted third party. It builds on digital signatures to create a chain of ownership for each coin, but addresses the doubleâ€‘spending problem by introducing a peer-to-peer â€œtimestamp serverâ€ that records all transactions in a continuously growing, cryptographically linked chain of blocks. Each block contains a set of transactions and a proof-of-work (PoW) that requires a SHA-256 hash to begin with a specified number of leading zero bits. The PoW ties the block to the amount of computational effort expended, making it infeasible for an attacker to alter past blocks without reâ€‘doing all subsequent work. The longest chain, i.e., the chain with the most cumulative PoW, is treated as the authoritative ledger; honest nodes will always extend this chain, while any divergent chain that fails to catch up is discarded.

The network operates with minimal coordination: new transactions are broadcast, nodes collect them into candidate blocks, and each node independently searches for a valid nonce. When a node finds a PoW, it broadcasts the block; other nodes accept it only if all contained transactions are valid and unspent. The first transaction in each block is a â€œcoinbaseâ€ transaction that creates new coins, providing an incentive for miners and a mechanism for initial coin distribution. Transaction fees are also incorporated, allowing the system to transition to an inflationâ€‘free model once a predetermined supply is reached. To conserve storage, transactions are hashed in a Merkle tree; only the root is stored in the block header (â‰ˆ80â€¯bytes), and spent transactions can be pruned once buried under enough subsequent blocks.

Privacy is maintained by keeping public keys anonymous and encouraging the use of a fresh key pair for each transaction, thereby preventing linkage of multiple payments to a single identity. The paper also introduces â€œsimplified payment verificationâ€ (SPV), where a lightweight client can verify a transaction by downloading only block headers and the Merkle branch that links the transaction to a block, reducing bandwidth and storage requirements. The author provides a probabilistic analysis of an attackerâ€™s chance to overtake the honest chain, showing that the probability of success drops exponentially with the number of blocks the attacker lags behind (e.g., a 10â€‘% attacker needs to be 5 blocks ahead to have a 0.1â€¯% chance of success).

In conclusion, the whitepaper presents a robust, unstructured peer-to-peer protocol that uses PoW to achieve consensus, prevent doubleâ€‘spending, and incentivize honest participation. The systemâ€™s simplicityâ€”no need for node identification or complex routingâ€”combined with its cryptographic guarantees, makes it a viable foundation for a global, trustless electronic cash network.

</details>

<details>
<summary><strong>gemma-3-12b-it</strong> â†’ bitcoin-paper</summary>

The Bitcoin whitepaper proposes a decentralized electronic cash system eliminating the need for trusted third parties. It utilizes digital signatures to establish coin ownership and solves the double-spending problem through a peer-to-peer network recording transactions in a tamper-evident ledger, structured as a continuously growing chain of blocks secured by a proof-of-work (PoW) mechanism. This PoW requires a computationally intensive hash preimage, making chain modification infeasible without controlling the majority of network hash power.

The network operates with minimal coordination, with nodes broadcasting transactions, assembling blocks, and competing to solve the PoW. The longest chain is authoritative, and nodes automatically adopt it. This, combined with PoW, provides a robust consensus mechanism, incentivized by new coin minting and transaction fees. The paper also details technical contributions like Merkle trees for efficient data pruning, simplified payment verification, and strategies for managing coin values.

Privacy is maintained through anonymous public keys and the encouragement of fresh key pairs per transaction. Ultimately, the whitepaper concludes that Bitcoin offers a secure, trustless, and scalable solution for electronic payments, relying on computational work for consensus rather than centralized authorities.

</details>

<details>
<summary><strong>gemma-3-12b-it</strong> â†’ Directorate A â€“ Expenditure â€“ Operations</summary>

The OLAF final report (Case OC/2021/0451/A1) investigates alleged serious misconduct and irregularities within the European Border and Coast Guard Agency (FRONTEX), focusing on possible illegal pushbacks of migrants and internal management failures. The investigation, opened on 3 May 2021 as a split of the earlier case OC/2020/0866/A1, examined the conduct of several FRONTEX personnel, including top managers and operational staff, and assessed whether the agency complied with its legal obligations under EU regulations and the Charter of Fundamental Rights.

Key findings show that multiple FRONTEX officials failed to follow standard operating procedures for serious incident reporting, excluded the Fundamental Rights Officer (FRO) from critical information, and engaged in intimidation, micromanagement, and possible conflicts of interest in recruitment and procurement. Digital forensic analysis, interviews with 20 witnesses, and openâ€‘source intelligence revealed that FRONTEX assets were involved in incidents that likely constituted illegal pushbacks, such as towing overcrowded migrant boats into Turkish territorial waters and leaving them adrift without propulsion. The FRO was systematically denied access to incident data, undermining the agencyâ€™s duty to monitor and protect fundamental rights.

The report concludes that the allegations are proven: FRONTEX personnelâ€™s actions hindered the agencyâ€™s ability to uphold its responsibilities, resulting in serious violations of fundamental rights and potential breaches of EU law. OLAFâ€™s evidence indicates that the agencyâ€™s internal processes were compromised, that the FRO was excluded from decisionâ€‘making, and that the agency failed to initiate or follow up on Serious Incident Reports (SIRs) when required. The findings highlight the need for corrective measures, including stricter adherence to SOPs, improved transparency, and stronger enforcement of Article 46 of the FRONTEX Regulation to prevent future violations.

</details>

<details>
<summary><strong>gemma-3-27b-it</strong> â†’ Directorate A â€“ Expenditure â€“ Operations</summary>

The OLAF final report (Case OC/2021/0451/A1) investigates serious irregularities and possible illegal pushbacks involving the European Border and Coast Guard Agency (FRONTEX). The investigation examined allegations of misconduct, intimidation, conflict of interest, procurement irregularities, and deliberate exclusion of the Fundamental Rights Officer (FRO) from incident reporting. Evidence was gathered through digital forensics, interviews, and review of documents and Serious Incident Reports (SIRs). Key findings show failures to follow procedures, obstruction of the FROâ€™s access to information, misclassification of SIRs, and relocation of assets to avoid witnessing incidents. The report concludes that the allegations are proven, the misconduct undermined FRONTEXâ€™s ability to protect fundamental rights, and recommends disciplinary action, review of procedures, and stronger oversight of the FROâ€™s role to ensure compliance with EU law and the protection of migrantsâ€™ rights.

</details>

<details>
<summary><strong>qwen3-vl-30b-a3b-instruct</strong> â†’ bitcoin-paper</summary>

The Bitcoin whitepaper introduces a decentralized electronic cash system that eliminates the need for trusted third parties. It proposes that transactions be recorded in a public, tamper-evident ledger built by a peer-to-peer network that uses proof-of-work (PoW) to timestamp and chain blocks of transactions. Each block contains a hash of the previous block, creating a longest-chain proof of the chronological order of events. Honest nodes, controlling the majority of CPU power, will naturally extend the longest chain, making it computationally infeasible for an attacker to rewrite history.

Transactions are represented as chains of digital signatures, allowing any party to verify ownership. To prevent double-spending without a central mint, all transactions are broadcast publicly and must be included in a block that passes PoW. The networkâ€™s minimal structureâ€”broadcasting messages on a best-effort basisâ€”enables nodes to join or leave freely while still maintaining consensus by following the longest chain. The PoW mechanism also serves as a one-CPU-one-vote system, ensuring that the majority of computational power determines the accepted history.

Bitcoinâ€™s incentive model rewards miners with newly minted coins (the â€œcoinbaseâ€ transaction) and transaction fees. This dual incentive encourages honest participation and, once a predetermined supply is reached, the system can become fully fee-based and inflation-free. The paper also discusses practical concerns such as disk-space reclamation via Merkle trees, simplified payment verification for lightweight clients, and privacy through the use of new key pairs for each transaction.

In conclusion, the whitepaper presents a robust, unstructured peer-to-peer network that uses PoW to secure a public ledger of transactions, thereby enabling trustless, non-reversible electronic cash. The design relies on honest majority consensus, minimal coordination, and economic incentives to maintain integrity and prevent double-spending.

</details>

<details>
<summary><strong>gemma-3-27b-it</strong> â†’ transformers-paper</summary>

The paper introduces the Transformer, a novel sequence-to-sequence architecture utilizing multi-head self-attention instead of recurrent or convolutional layers. This design enables greater parallelism, reducing training time and improving translation quality, achieving state-of-the-art BLEU scores of 28.4 (EN-DE) and 41.8 (EN-FR) on WMT 2014 benchmarks with significantly reduced computational cost (3.5 days on eight P100 GPUs). 

The Transformer's architecture consists of stacked encoder and decoder layers with multi-head self-attention, position-wise feed-forward networks, residual connections, layer normalization, and scaled dot-product attention with a âˆšdk scaling factor. Positional encodings are used to provide information about the relative positions of tokens. The paper details the impact of varying hyperparameters like the number of heads, dimensions, and dropout rates on performance.

Beyond translation, the Transformer demonstrates versatility by achieving strong results on other tasks, such as parsing. A 4-layer model achieves 91.3 F1 on the Penn Treebank (WSJ section 23), surpassing RNN-based parsers, and 92.7 F1 in a semi-supervised setting. 

The authors conclude that attention-only models are both efficient and adaptable, suggesting future work involving multimodal inputs and restricted attention mechanisms for large datasets. The code is available on GitHub.

</details>

<details>
<summary><strong>gpt-oss-20b</strong> â†’ transformers-paper</summary>

The paper â€œAttention Is All You Needâ€ introduces the Transformer, a novel sequenceâ€‘toâ€‘sequence architecture that replaces recurrent and convolutional layers with a stack of multiâ€‘head selfâ€‘attention modules. The model consists of an encoder and a decoder, each built from identical layers that contain a multiâ€‘head attention subâ€‘layer followed by a positionâ€‘wise feedâ€‘forward network, with residual connections and layer normalization. Selfâ€‘attention is implemented as scaled dotâ€‘product attention, and the multiâ€‘head design allows the model to attend to information from multiple representation subâ€‘spaces simultaneously. Positional information is injected via sinusoidal encodings added to token embeddings, enabling the network to capture relative positions without recurrence.

Training on the WMT 2014 Englishâ€‘toâ€‘German and Englishâ€‘toâ€‘French datasets, the Transformer achieves stateâ€‘ofâ€‘theâ€‘art BLEU scores while requiring far less training time. The base model (6 layers, 512â€‘dimensional embeddings, 8 attention heads) reaches 27.3 BLEU on ENâ€‘DE and 38.1 BLEU on ENâ€‘FR after 12â€¯h on 8 P100 GPUs. The larger â€œbigâ€ model (6 layers, 1024â€‘dimensional embeddings, 16 heads, 4096â€‘dimensional feedâ€‘forward) attains 28.4 BLEU on ENâ€‘DE and 41.8 BLEU on ENâ€‘FR after 3.5â€¯days, surpassing all previous singleâ€‘model results and even outperforming ensembles at a fraction of their computational cost. The paper also demonstrates the Transformerâ€™s generality by applying it to English constituency parsing, achieving 92.7â€¯F1 on the WSJ test setâ€”competitive with the best RNNâ€‘based parsersâ€”using only a 4â€‘layer model trained on 40â€¯k sentences.

Key contributions include: (1) a fully attentionâ€‘based architecture that eliminates sequential dependencies, enabling massive parallelism; (2) a scalable multiâ€‘head attention mechanism that preserves expressive power while keeping computational cost comparable to singleâ€‘head attention; (3) a simple yet effective positional encoding scheme; and (4) empirical evidence that the Transformer outperforms recurrent and convolutional models on translation and parsing tasks with significantly reduced training time. The authors release the implementation in Tensor2Tensor, encouraging further research into attentionâ€‘only models for diverse modalities.

</details>

<details>
<summary><strong>qwen3-vl-30b-a3b-instruct</strong> â†’ transformers-paper</summary>

The paper introduces the Transformer, a novel sequence-to-sequence architecture that replaces recurrent and convolutional layers with multi-head self-attention and position-wise feed-forward networks. By dispensing with recurrence, the model achieves far greater parallelism during training and inference, reducing training time while improving translation quality. The authors demonstrate that a single Transformer model can surpass previous state-of-the-art systems on both WMT 2014 English-to-German (28.4 BLEU) and English-to-French (41.8 BLEU) tasks, achieving these scores in only 12 hours and 3.5 days of training on eight P100 GPUs, respectivelyâ€”substantially less computational cost than prior ensembles.

Key architectural details include six identical encoder and decoder layers, each with residual connections and layer normalization, eight attention heads with 64-dimensional key/value vectors, and a 512-dimensional model size (doubling to 1024 for the "big" model). The Transformer uses sinusoidal positional encodings to inject order information, and applies dropout, label smoothing, and a warm-up learning-rate schedule to regularize training. Ablation studies show that the number of heads, key/value dimensions, and dropout rates significantly affect performance, while learned positional embeddings perform comparably to sinusoidal ones.

Beyond machine translation, the authors validate the Transformerâ€™s generality by applying it to English constituency parsing. A 4-layer Transformer trained on the Penn Treebank achieves 91.3 F1 on WSJ section 23, matching or exceeding many RNN-based parsers, and reaches 92.7 F1 in a semi-supervised setting with 17 M additional sentences. The paper concludes that attention-only models open new avenues for efficient, high-quality sequence transduction across modalities, and outlines future work on local attention mechanisms and multimodal extensions.

</details>

<details>
<summary><strong>qwen3-vl-30b-a3b-instruct</strong> â†’ Directorate A â€“ Expenditure â€“ Operations</summary>

The OLAF final report (Case OC/2021/0451/A1) investigates serious irregularities and alleged misconduct within the European Border and Coast Guard Agency (FRONTEX), focusing on possible illegal pushbacks of migrants and the agencyâ€™s internal management failures. The investigation, opened on 3 May 2021 as a split of an earlier case, examined allegations that FRONTEX staff and senior managers had ignored or concealed incidents involving potential violations of fundamental rights, failed to follow Standard Operating Procedures (SOPs) for Serious Incident Reporting (SIR), and obstructed the Fundamental Rights Officer (FRO) from accessing relevant information. Evidence was gathered through digital forensics, interviews with 20 witnesses, analysis of open-source material, and inspection of FRONTEX premises. Key findings include: (i) multiple SIRs were misclassified or not escalated to Category 4, thereby excluding the FRO from oversight; (ii) several senior officials deliberately withheld or distorted information, undermining the agencyâ€™s duty to protect fundamental rights; (iii) FRONTEX failed to initiate or follow up on SIRs that raised serious concerns about non-refoulement and other EU legal obligations; (iv) the agencyâ€™s internal reporting lines were manipulated to avoid political scrutiny and to protect certain personnel. The report concludes that the allegations are proven, that FRONTEXâ€™s actions constitute serious misconduct and violations of EU regulations and the Charter of Fundamental Rights, and that the agencyâ€™s capacity to monitor and respect fundamental rights at the external borders has been significantly impaired.

</details>

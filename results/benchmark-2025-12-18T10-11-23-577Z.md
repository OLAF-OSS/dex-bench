# Benchmark Results

**Run ID:** `benchmark-2025-12-18T10-11-23-577Z`
**Timestamp:** 12/18/2025, 10:11:23 AM
**Models:** 3
**Documents:** 3
**Categories:** summarization

## Table of Contents

- [Summarization Benchmark](#summarization-benchmark)

## Summarization Benchmark

Tests LLM summarization capabilities with document analysis.

### Results

| Model | Document | Doc Tokens | Time | Input Tokens | Output Tokens | Total Tokens | Tok/s | Status |
|-------|----------|------------|------|--------------|---------------|--------------|-------|--------|
| a100/gemma-3-12b | bitcoin-paper | 4,626 | 37.6s | 4,722 | 265 | 4,987 | 7.0 | ‚úÖ |
| a100/gemma-3-12b | transformers-paper | 10,200 | 47.9s | 10,297 | 247 | 10,544 | 5.2 | ‚úÖ |
| a100/gemma-3-27b | bitcoin-paper | 4,626 | 35.9s | 4,722 | 254 | 4,976 | 7.1 | ‚úÖ |
| a100/gemma-3-12b | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 1m 29s | 41,768 | 359 | 42,127 | 4.0 | ‚úÖ |
| a100/gemma-3-27b | transformers-paper | 10,200 | 47.9s | 10,297 | 257 | 10,554 | 5.4 | ‚úÖ |
| a100/gemma-3-27b | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 51.5s | 41,768 | 323 | 42,091 | 6.3 | ‚úÖ |
| a100/qwen3-vl-30b | bitcoin-paper | 4,626 | 1m 13.4s | 4,722 | 265 | 4,987 | 3.6 | ‚úÖ |
| a100/qwen3-vl-30b | transformers-paper | 10,200 | 2m 23.7s | 10,297 | 247 | 10,544 | 1.7 | ‚úÖ |
| a100/qwen3-vl-30b | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 3m 18s | 41,768 | 323 | 42,091 | 1.6 | ‚úÖ |

### Statistics

- **Total Duration:** 12m 5.3s
- **Average Duration:** 1m 20.5s
- **Total Input Tokens:** 170,361
- **Total Output Tokens:** 2,540
- **Average Tokens/s:** 4.7
- **‚ö° Fastest:** a100/gemma-3-27b on bitcoin-paper.md (35.9s)
- **üê¢ Slowest:** a100/qwen3-vl-30b on Directorate A ‚Äì Expenditure ‚Äì Operations.md (3m 18s)

### Model Averages

| Model | Average Time |
|-------|--------------|
| a100/gemma-3-27b | 45.1s |
| a100/gemma-3-12b | 58.2s |
| a100/qwen3-vl-30b | 2m 18.4s |

### Summaries

<details>
<summary><strong>a100/gemma-3-12b</strong> ‚Üí bitcoin-paper</summary>

This document introduces Bitcoin, a proposed peer-to-peer electronic cash system designed to enable online payments directly between parties without the need for trusted third parties like financial institutions. The core problem addressed is the double-spending issue in digital currencies, traditionally solved by centralized authorities. Bitcoin proposes a decentralized solution using a peer-to-peer network that timestamps transactions by creating a chain of "proof-of-work," requiring computational effort to add new blocks to the chain. The longest chain, representing the most computational work, is considered the valid history of transactions.

The system functions by broadcasting new transactions, collecting them into blocks, and requiring nodes to solve a computationally difficult proof-of-work problem for each block. This proof-of-work secures the network, making it computationally impractical for attackers to alter past transactions. Incentives are built in through the creation of new coins awarded to nodes that create blocks, and transaction fees. The document details the network's operation, including simplified payment verification and addresses privacy concerns by suggesting the use of new key pairs for each transaction. Finally, it analyzes the probability of an attacker successfully manipulating the chain, demonstrating that the odds diminish exponentially as the honest network grows. The paper concludes that Bitcoin offers a robust and decentralized system for electronic transactions, relying on cryptographic proof rather than trust.

</details>

<details>
<summary><strong>a100/gemma-3-12b</strong> ‚Üí transformers-paper</summary>

This document introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation, that eschews recurrence and convolutions entirely, relying solely on attention mechanisms. The authors demonstrate that the Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks, surpassing previous models while requiring significantly less training time and computational resources. The model's architecture consists of stacked self-attention and feed-forward layers in both the encoder and decoder, enabling greater parallelization and efficient learning of long-range dependencies.

Experiments show the Transformer generalizes well to other tasks, successfully applied to English constituency parsing. The paper details the model's architecture, including scaled dot-product attention and multi-head attention, and provides a comparison of its computational complexity and path lengths to recurrent and convolutional networks. Results indicate that the Transformer achieves superior performance with reduced training costs, establishing a new state-of-the-art in machine translation and demonstrating the potential of attention-based models for various sequence transduction problems. Visualizations of the attention mechanism reveal that different attention heads learn to perform distinct tasks, including capturing long-distance dependencies and resolving anaphora, suggesting the model learns meaningful linguistic structures.

</details>

<details>
<summary><strong>a100/gemma-3-27b</strong> ‚Üí bitcoin-paper</summary>

This document introduces Bitcoin, a proposed peer-to-peer electronic cash system designed to enable online payments directly between parties without the need for trusted third parties like financial institutions. The core problem addressed is the double-spending issue in digital currencies, traditionally solved by centralized authorities. Bitcoin proposes a decentralized solution using a peer-to-peer network that timestamps transactions by creating a chain of "proof-of-work," requiring computational effort to add new blocks to the chain. The longest chain, representing the most computational work, is considered the valid history of transactions.

The system functions through a network where nodes collect transactions into blocks, solve a computationally intensive proof-of-work problem for each block, and broadcast the block to the network. Nodes accept blocks containing valid, unspent transactions and continue building upon the longest chain. Incentives are provided through newly created coins awarded to nodes that create blocks, and transaction fees. The document details mechanisms for simplified payment verification, privacy considerations (using new key pairs per transaction), and analyzes the probability of an attacker successfully altering the transaction history, demonstrating that it diminishes exponentially with the number of blocks added. Ultimately, the paper argues that this system can establish a secure and trustless electronic payment system relying on cryptographic proof and distributed consensus.

</details>

<details>
<summary><strong>a100/gemma-3-12b</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This document is a final report from OLAF (European Anti-Fraud Office) detailing an investigation initiated in September 2021, stemming from allegations of irregularities and potential misconduct within the European Border and Coast Guard Agency (FRONTEX). The investigation, initially opened in November 2020 concerning potential involvement in illegal pushbacks, was split to broaden the scope and expedite matters. The core of the report focuses on the actions and decisions of several FRONTEX personnel regarding the handling of serious incidents, particularly those involving potential violations of fundamental rights, specifically concerning alleged pushbacks of migrants. The investigation revealed a pattern of failures, including the exclusion of the Fundamental Rights Officer (FRO) from assessments and handling of incidents, a lack of initiation of Serious Incident Reports, and decisions to relocate assets to avoid witnessing potential violations. The report highlights instances where information was withheld from the FRO, classifications of incidents were manipulated, and a general reluctance to address concerns regarding the Greek authorities' actions.

The report concludes that the actions of multiple individuals within FRONTEX hindered the agency's ability to fully comply with its responsibilities regarding fundamental rights, citing failures in following procedures, a lack of loyalty to the Union, and inadequate managerial oversight. Specific failings included the mishandling of SIRs related to incidents involving the Hellenic Coast Guard, the relocation of FSA assets to avoid witnessing potential pushbacks, and a general lack of transparency and cooperation with the FRO. The report details numerous instances of internal communication and decision-making processes that contributed to these failures, ultimately concluding that the individuals involved committed serious misconduct and other irregularities. The report meticulously outlines the sequence of events, communications, and decisions made throughout the investigation, providing a detailed account of the shortcomings within FRONTEX's operational and management practices.

</details>

<details>
<summary><strong>a100/gemma-3-27b</strong> ‚Üí transformers-paper</summary>

This document introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation, that eschews recurrence and convolutions entirely, relying solely on attention mechanisms. The authors demonstrate that the Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks, surpassing previous models while requiring significantly less training time and computational resources. The model's architecture consists of stacked self-attention and feed-forward layers in both the encoder and decoder, enabling greater parallelization and efficient learning of long-range dependencies.

Experiments show the Transformer generalizes well to other tasks, successfully applied to English constituency parsing. The paper details the model's architecture, including scaled dot-product attention and multi-head attention, and provides a comparison of its computational complexity and path lengths to recurrent and convolutional networks. Results indicate that the Transformer's parallelizability and ability to model long-range dependencies effectively contribute to its superior performance, achieving BLEU scores of 28.4 on English-to-German and 41.8 on English-to-French translation, with significantly reduced training costs compared to existing models. The authors also present visualizations of the attention mechanism, revealing insights into how the model learns syntactic and semantic relationships within sentences.

</details>

<details>
<summary><strong>a100/gemma-3-27b</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This document is a final report from OLAF (European Anti-Fraud Office) detailing an investigation initiated in September 2021, stemming from a fraud notification system report concerning the European Border and Coast Guard Agency (FRONTEX). The investigation, initially opened in November 2020, focused on potential misconduct and irregularities related to FRONTEX's operations, including allegations of involvement in and cover-up of illegal pushbacks. The investigation was split to broaden its scope and expedite matters. The report outlines investigative activities, including information collection, interviews with 20 witnesses, digital forensic operations, and analysis of evidence.

The report details numerous instances of failures by FRONTEX personnel, including a failure to follow procedures, a lack of loyalty, and deficiencies in managerial responsibilities. Specific issues highlighted include the mishandling of Serious Incident Reports (SIRs), particularly concerning incidents involving potential pushbacks by Greek authorities, where information was suppressed, classifications were manipulated, and the Fundamental Rights Officer (FRO) was excluded from investigations. The report also addresses concerns about the relocation of FSA assets to avoid witnessing potential human rights violations, and instances of staff disclosing sensitive information and providing misleading information to stakeholders. Ultimately, OLAF concludes that the actions of the individuals investigated hindered FRONTEX's ability to comply with its responsibilities, particularly regarding the protection of fundamental rights at the EU's external borders. The report details specific incidents, the actions taken (or not taken) by various individuals, and the justifications provided, revealing a pattern of attempts to avoid scrutiny and potentially conceal wrongdoing within the agency.

</details>

<details>
<summary><strong>a100/qwen3-vl-30b</strong> ‚Üí bitcoin-paper</summary>

This document introduces Bitcoin, a proposed peer-to-peer electronic cash system designed to enable online payments directly between parties without the need for trusted third parties like financial institutions. The core problem addressed is the double-spending issue in digital currencies, traditionally solved by centralized authorities. Bitcoin proposes a decentralized solution using a peer-to-peer network that timestamps transactions by creating a chain of "proof-of-work," requiring computational effort to add new blocks to the chain. The longest chain, representing the most computational work, is considered the valid history of transactions.

The system functions by broadcasting new transactions, collecting them into blocks, and requiring nodes to solve a computationally difficult proof-of-work problem for each block. This proof-of-work secures the network, making it computationally impractical for attackers to alter past transactions. Incentives are built in through the creation of new coins awarded to nodes that create blocks, and transaction fees. The document details the network's operation, including simplified payment verification and addresses privacy concerns by suggesting the use of new key pairs for each transaction. Finally, it analyzes the probability of an attacker successfully manipulating the chain, demonstrating that the odds diminish exponentially as the honest network grows. The paper concludes that Bitcoin offers a robust and decentralized system for electronic transactions, relying on cryptographic proof rather than trust.

</details>

<details>
<summary><strong>a100/qwen3-vl-30b</strong> ‚Üí transformers-paper</summary>

This document introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation, that eschews recurrence and convolutions entirely, relying solely on attention mechanisms. The authors demonstrate that the Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks, surpassing previous models while requiring significantly less training time and computational resources. The model's architecture consists of stacked self-attention and feed-forward layers in both the encoder and decoder, enabling greater parallelization and efficient learning of long-range dependencies.

Experiments show the Transformer generalizes well to other tasks, successfully applied to English constituency parsing. The paper details the model's architecture, including scaled dot-product attention and multi-head attention, and provides a comparison of its computational complexity and path lengths to recurrent and convolutional networks. Results indicate that the Transformer achieves superior performance with reduced training costs, establishing a new state-of-the-art in machine translation and demonstrating the potential of attention-based models for various sequence transduction problems. Visualizations of the attention mechanism reveal that different attention heads learn to perform distinct tasks, including capturing long-distance dependencies and resolving anaphora, suggesting the model learns meaningful linguistic structures.

</details>

<details>
<summary><strong>a100/qwen3-vl-30b</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This document is a final report from OLAF (European Anti-Fraud Office) detailing an investigation initiated in September 2021, stemming from a fraud notification system report concerning the European Border and Coast Guard Agency (FRONTEX). The investigation, initially opened in November 2020, focused on potential misconduct and irregularities related to FRONTEX's operations, including allegations of involvement in and cover-up of illegal pushbacks. The investigation was split to broaden its scope and expedite matters. The report outlines investigative activities, including information collection, interviews with 20 witnesses, digital forensic operations, and analysis of evidence.

The report details numerous instances of failures by FRONTEX personnel, including a failure to follow procedures, a lack of loyalty, and deficiencies in managerial responsibilities. Specific issues highlighted include the mishandling of Serious Incident Reports (SIRs), particularly concerning incidents involving potential pushbacks by Greek authorities, where information was suppressed, classifications were manipulated, and the Fundamental Rights Officer (FRO) was excluded from investigations. The report also addresses concerns about the relocation of FSA assets to avoid witnessing potential human rights violations, and instances of staff disclosing sensitive information and providing misleading information to stakeholders. Ultimately, OLAF concludes that the actions of the individuals investigated hindered FRONTEX's ability to comply with its responsibilities, particularly regarding the protection of fundamental rights at the EU's external borders. The report details specific incidents, the actions taken (or not taken) by various individuals, and the justifications provided, revealing a pattern of attempts to avoid scrutiny and potentially conceal wrongdoing within the agency.

</details>

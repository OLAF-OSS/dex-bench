# Benchmark Results

**Run ID:** `benchmark-2025-12-18T10-27-40-969Z`
**Timestamp:** 12/18/2025, 10:27:40 AM
**Models:** 3
**Documents:** 3
**Categories:** summarization

## Table of Contents

- [Summarization Benchmark](#summarization-benchmark)

## Summarization Benchmark

Tests LLM summarization capabilities with document analysis.

### Results

| Model | Document | Doc Tokens | Time | Input Tokens | Output Tokens | Total Tokens | Tok/s | Status |
|-------|----------|------------|------|--------------|---------------|--------------|-------|--------|
| a100/gemma-3-12b | bitcoin-paper | 4,626 | 49.2s | 4,722 | 254 | 4,976 | 5.2 | ‚úÖ |
| a100/gemma-3-12b | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 1m 2.5s | 41,768 | 310 | 42,078 | 5.0 | ‚úÖ |
| a100/gemma-3-12b | transformers-paper | 10,200 | 1m 6.2s | 10,297 | 235 | 10,532 | 3.5 | ‚úÖ |
| a100/gemma-3-27b | bitcoin-paper | 4,626 | 44.4s | 4,722 | 254 | 4,976 | 5.7 | ‚úÖ |
| a100/gemma-3-27b | transformers-paper | 10,200 | 48.3s | 10,297 | 257 | 10,554 | 5.3 | ‚úÖ |
| a100/gemma-3-27b | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 1m 3.5s | 41,768 | 308 | 42,076 | 4.8 | ‚úÖ |
| a100/qwen3-vl-30b | bitcoin-paper | 4,626 | 6m 54.5s | 4,722 | 254 | 4,976 | 0.6 | ‚úÖ |
| a100/qwen3-vl-30b | transformers-paper | 10,200 | 7m 30.4s | 10,297 | 257 | 10,554 | 0.6 | ‚úÖ |
| a100/qwen3-vl-30b | Directorate A ‚Äì Expenditure ‚Äì Operations | 41,672 | 7m 58.4s | 41,768 | 308 | 42,076 | 0.6 | ‚úÖ |

### Statistics

- **Total Duration:** 27m 57.7s
- **Average Duration:** 3m 6.4s
- **Total Input Tokens:** 170,361
- **Total Output Tokens:** 2,437
- **Average Tokens/s:** 3.5
- **‚ö° Fastest:** a100/gemma-3-27b on bitcoin-paper.md (44.4s)
- **üê¢ Slowest:** a100/qwen3-vl-30b on Directorate A ‚Äì Expenditure ‚Äì Operations.md (7m 58.4s)

### Model Averages

| Model | Average Time |
|-------|--------------|
| a100/gemma-3-27b | 52.1s |
| a100/gemma-3-12b | 59.3s |
| a100/qwen3-vl-30b | 7m 27.7s |

### Summaries

<details>
<summary><strong>a100/gemma-3-12b</strong> ‚Üí bitcoin-paper</summary>

This document introduces Bitcoin, a proposed peer-to-peer electronic cash system designed to enable online payments directly between parties without the need for trusted third parties like financial institutions. The core problem addressed is the double-spending issue in digital currencies, traditionally solved by centralized authorities. Bitcoin proposes a decentralized solution using a peer-to-peer network that timestamps transactions by creating a chain of "proof-of-work," requiring computational effort to add new blocks to the chain. The longest chain, representing the most computational work, is considered the valid history of transactions.

The system functions through a network where nodes collect transactions into blocks, solve a computationally intensive proof-of-work problem for each block, and broadcast the block to the network. Nodes accept blocks containing valid, unspent transactions and continue building upon the longest chain. Incentives are provided through newly created coins awarded to nodes that create blocks, and transaction fees. The document details mechanisms for simplified payment verification, privacy considerations (using new key pairs per transaction), and analyzes the probability of an attacker successfully altering the transaction history, demonstrating that it diminishes exponentially with the number of blocks added. Ultimately, the paper argues that this system can establish a secure and trustless electronic payment system relying on cryptographic proof and distributed consensus.

</details>

<details>
<summary><strong>a100/gemma-3-12b</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This document is a final report from OLAF (European Anti-Fraud Office) detailing an investigation initiated in September 2021, stemming from a fraud notification system report concerning the European Border and Coast Guard Agency (FRONTEX). The investigation, initially opened in November 2020, focused on potential misconduct and irregularities related to FRONTEX's operations, including allegations of involvement in and cover-up of illegal pushbacks. The case was split to expedite matters and broaden the scope of investigation. The report outlines investigative activities including information collection, interviews with 20 witnesses, digital forensic operations, and analysis of evidence.

The investigation revealed significant failings by several FRONTEX personnel, including Klaus-Michael Staude, Univ.Prof. Joachim Ring, Bautista Barrera Cabanillas, and others, who hindered FRONTEX's ability to comply with fundamental rights responsibilities. These failings included failures to follow procedures, breaches of duty of loyalty, and inadequate managerial oversight. Specific incidents, such as the handling of SIRs (Serious Incident Reports) related to pushback allegations, demonstrated a pattern of obstructing the Fundamental Rights Officer's (FRO) access to information and hindering thorough investigations. The report concludes that the evidence supports the allegations of serious misconduct and irregularities, highlighting a systemic failure to uphold fundamental rights within FRONTEX and a lack of accountability among key personnel. The report details numerous instances where information was withheld from the FRO, classifications of incidents were manipulated, and actions were taken to avoid scrutiny of potential human rights violations.

</details>

<details>
<summary><strong>a100/gemma-3-12b</strong> ‚Üí transformers-paper</summary>

This document introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation, that eschews recurrence and convolutions entirely, relying solely on attention mechanisms. The authors demonstrate that the Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks, surpassing previous models while requiring significantly less training time and computational resources. The model's architecture consists of stacked self-attention and feed-forward layers in both the encoder and decoder, allowing for greater parallelization and efficient learning of long-range dependencies.

Experiments show the Transformer generalizes well to other tasks, successfully applied to English constituency parsing. The paper details the model's architecture, including scaled dot-product attention and multi-head attention, and provides a comparison of its computational complexity and path lengths to recurrent and convolutional networks. Results indicate that the Transformer achieves superior performance with reduced training costs, establishing a new state-of-the-art in machine translation and demonstrating the potential of attention-based models for various sequence transduction problems. The authors also include visualizations of the attention mechanism, revealing insights into how the model learns syntactic and semantic relationships within sentences.

</details>

<details>
<summary><strong>a100/gemma-3-27b</strong> ‚Üí bitcoin-paper</summary>

This document introduces Bitcoin, a proposed peer-to-peer electronic cash system designed to enable online payments directly between parties without the need for trusted third parties like financial institutions. The core problem addressed is the double-spending issue in digital currencies, traditionally solved by centralized authorities. Bitcoin proposes a decentralized solution using a peer-to-peer network that timestamps transactions by creating a chain of "proof-of-work," requiring computational effort to add new blocks to the chain. The longest chain, representing the most computational work, is considered the valid history of transactions.

The system functions through a network where nodes collect transactions into blocks, solve a computationally intensive proof-of-work problem for each block, and broadcast the block to the network. Nodes accept blocks containing valid, unspent transactions and continue building upon the longest chain. Incentives are provided through newly created coins awarded to nodes that create blocks, and transaction fees. The document details mechanisms for simplified payment verification, privacy considerations (using new key pairs per transaction), and analyzes the probability of an attacker successfully altering the transaction history, demonstrating that it diminishes exponentially with the number of blocks added. Ultimately, the paper argues that this system can establish a secure and trustless electronic payment system relying on cryptographic proof and distributed consensus.

</details>

<details>
<summary><strong>a100/gemma-3-27b</strong> ‚Üí transformers-paper</summary>

This document introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation, that eschews recurrence and convolutions entirely, relying solely on attention mechanisms. The authors demonstrate that the Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks, surpassing previous models while requiring significantly less training time and computational resources. The model's architecture consists of stacked self-attention and feed-forward layers in both the encoder and decoder, enabling greater parallelization and efficient learning of long-range dependencies.

Experiments show the Transformer generalizes well to other tasks, successfully applied to English constituency parsing. The paper details the model's architecture, including scaled dot-product attention and multi-head attention, and provides a comparison of its computational complexity and path lengths to recurrent and convolutional networks. Results indicate that the Transformer's parallelizability and ability to model long-range dependencies effectively contribute to its superior performance, achieving BLEU scores of 28.4 on English-to-German and 41.8 on English-to-French translation, with significantly reduced training costs compared to existing models. The authors also present visualizations of the attention mechanism, revealing insights into how the model learns syntactic and semantic relationships within sentences.

</details>

<details>
<summary><strong>a100/gemma-3-27b</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This document is a final report from OLAF (European Anti-Fraud Office) detailing an investigation initiated in September 2021, stemming from allegations of irregularities and potential misconduct within the European Border and Coast Guard Agency (FRONTEX). The investigation, initially opened in November 2020 concerning potential involvement in illegal pushbacks, was split to broaden the scope and expedite matters. The core of the report focuses on the actions and decisions of several FRONTEX personnel regarding the handling of incidents, particularly those involving potential violations of fundamental rights, specifically concerning alleged pushbacks of migrants. The investigation revealed failures in adhering to standard operating procedures, a lack of transparency, and a reluctance to fully cooperate with the Fundamental Rights Officer (FRO) in investigating these incidents.

The report details specific instances where FRONTEX staff, including high-ranking officials, allegedly prioritized political considerations and operational expediency over upholding fundamental rights and proper reporting procedures. Key findings include the misclassification of serious incident reports, the withholding of information from the FRO, and decisions to relocate assets to avoid witnessing potential human rights violations. OLAF concludes that the actions of the individuals investigated hindered FRONTEX's ability to fully comply with its responsibilities regarding the protection of fundamental rights at the EU's external borders. The report outlines a series of failures, including a lack of follow-up actions, inadequate communication, and a general reluctance to address concerns raised by the FRO, ultimately contributing to a climate where potential violations of fundamental rights were not adequately investigated or addressed.

</details>

<details>
<summary><strong>a100/qwen3-vl-30b</strong> ‚Üí bitcoin-paper</summary>

This document introduces Bitcoin, a proposed peer-to-peer electronic cash system designed to enable online payments directly between parties without the need for trusted third parties like financial institutions. The core problem addressed is the double-spending issue in digital currencies, traditionally solved by centralized authorities. Bitcoin proposes a decentralized solution using a peer-to-peer network that timestamps transactions by creating a chain of "proof-of-work," requiring computational effort to add new blocks to the chain. The longest chain, representing the most computational work, is considered the valid history of transactions.

The system functions through a network where nodes collect transactions into blocks, solve a computationally intensive proof-of-work problem for each block, and broadcast the block to the network. Nodes accept blocks containing valid, unspent transactions and continue building upon the longest chain. Incentives are provided through newly created coins awarded to nodes that create blocks, and transaction fees. The document details mechanisms for simplified payment verification, privacy considerations (using new key pairs per transaction), and analyzes the probability of an attacker successfully altering the transaction history, demonstrating that it diminishes exponentially with the number of blocks added. Ultimately, the paper argues that this system can establish a secure and trustless electronic payment system relying on cryptographic proof and distributed consensus.

</details>

<details>
<summary><strong>a100/qwen3-vl-30b</strong> ‚Üí transformers-paper</summary>

This document introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation, that eschews recurrence and convolutions entirely, relying solely on attention mechanisms. The authors demonstrate that the Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks, surpassing previous models while requiring significantly less training time and computational resources. The model's architecture consists of stacked self-attention and feed-forward layers in both the encoder and decoder, enabling greater parallelization and efficient learning of long-range dependencies.

Experiments show the Transformer generalizes well to other tasks, successfully applied to English constituency parsing. The paper details the model's architecture, including scaled dot-product attention and multi-head attention, and provides a comparison of its computational complexity and path lengths to recurrent and convolutional networks. Results indicate that the Transformer's parallelizability and ability to model long-range dependencies effectively contribute to its superior performance, achieving BLEU scores of 28.4 on English-to-German and 41.8 on English-to-French translation, with significantly reduced training costs compared to existing models. The authors also present visualizations of the attention mechanism, revealing insights into how the model learns syntactic and semantic relationships within sentences.

</details>

<details>
<summary><strong>a100/qwen3-vl-30b</strong> ‚Üí Directorate A ‚Äì Expenditure ‚Äì Operations</summary>

This document is a final report from OLAF (European Anti-Fraud Office) detailing an investigation initiated in September 2021, stemming from allegations of irregularities and potential misconduct within the European Border and Coast Guard Agency (FRONTEX). The investigation, initially opened in November 2020 concerning potential involvement in illegal pushbacks, was split to broaden the scope and expedite matters. The core of the report focuses on the actions and decisions of several FRONTEX personnel regarding the handling of incidents, particularly those involving potential violations of fundamental rights, specifically concerning alleged pushbacks of migrants. The investigation revealed failures in adhering to standard operating procedures, a lack of transparency, and a reluctance to fully cooperate with the Fundamental Rights Officer (FRO) in investigating these incidents.

The report details specific instances where FRONTEX staff, including high-ranking officials, allegedly prioritized political considerations and operational expediency over upholding fundamental rights and proper reporting procedures. Key findings include the misclassification of serious incident reports, the withholding of information from the FRO, and decisions to relocate assets to avoid witnessing potential human rights violations. OLAF concludes that the actions of the individuals investigated hindered FRONTEX's ability to fully comply with its responsibilities regarding the protection of fundamental rights at the EU's external borders. The report outlines a series of failures, including a lack of follow-up actions, inadequate communication, and a general reluctance to address concerns raised by the FRO, ultimately contributing to a climate where potential violations of fundamental rights were not adequately investigated or addressed.

</details>
